● 
뉴럴넷의 한 컴포넌트는 ~~처럼 생김.
계속 층을 쌓아가서 깊어지면 딥뉴럴넷(DNN)이라 한다.

Q: 사람의 뇌신경을 닮은 뉴럴넷이니 사람이 하는거 다 할 수 있지 않을까?
A: 잘 안됨. 세 가지 문제가 있다.
1. Underfitting - 학습이 잘 안됨
2. SLow         - 학습이 느림
3. Overfitting  - 겨우 되어도 융통성이 없음

●●●●
1-> Underfitting 해결과정
뉴럴넷의 학습방법은 Back propagation (=뒤로 전달)
뭐를 전달하는가?
-> 현재 내가 틀린정도를 '미분(기울기)'한 거
미분하고, 곱하고, 더하고를 역방향으로 반복하며 업데이트한다.
그런데 문제는? 우리가 activation함수로 sigmoid를 쓰면 가운데는 기울기가 있지만 그외는 기울기가 0이다.
-> 중간에 0을 곱하면서 뒤로 전달하면 Vanishing gradient현상이 일어난다
    Vanishing gradient현상: 레이어가 깊을 수록 업데이트가 사라져간다.
                            그래서 fitting이 잘 안됨.(Underfitting)
=> 사그라드는 sigmoid대신 죽지않는 activation func을 쓰자 = ReLU(Rectified Linear Units)
    ReLU: 양의 구간에서 전부 미분값이 1이다.

문제: 전달하다가, 사그라져 버린다(Vanishing gradient)
해결: sigmoid -> ReLU = 뒤로전달 OK!

●●●●
2-> Slow 해결과정
Q: 기존 뉴럴넷이 가중치 parameter들을 최적화(optimize)하는 방법은?
A: Gradient Decent
loss func의 현 가중치에서의 기울기(gradient)를 구해서 loss를 줄이는 방향으로 업데이트해 나간다.
    loss func: 쉽게말해 "틀린 정도"

방법:
    1. 현재 가진 weight에서 내가 가진 데이터를 "다" 넣으면 전체 에러가 계산된다.
    2. 여기에서 미분하면 에러를 줄이는 방향(현재위치에서 기울기 반대방향)을 알 수 있다.
    3. 이 방향으로 정해진 스텝량(learning rate)을 곱해서 weight을 이동시킨다.
    4. 1ㅡ2ㅡ3을 반복한다.

weight의 업데이트 = decent x learning rate x gradient
    decent = 에러를 낮추는 방향
    learning rate = 한발자국 크기
    gradient = 현 지점의 기울기

그러나, 내가 가진 데이터가 몇 억건인데 한발자국 갈 때마다 이걸 다 넣으니 느려진다.
Q: GD보다 빠른 옵티마이저는 없을까?
A: Stochastic Gradient Decent(SGD)
    SGD: 느린 완벽보다 조금만 훑어보고 빨리 가자.

GD : 전부다 읽고나서 최적의 1스텝을 이동한다.
    즉, 모든 걸 계산(1시간) 후 최적의 한스텝
    6스텝 * 1시간 = 6시간
    -> 최적인데 너무 느리다

SGD: mini-batch로 작은 토막마다 일단 1스텝 간다.
    즉, 일부만 검토(5분)
    틀려도 일단 간다. 빠른 스텝!
    11스텝 * 5분 = 55분 < 1시간
    -> 조금 헤매도 어쨋든 목적지 인근에 아주 빨리 갔다.

Q: 걸음마다 batch로 전부다 계산하려니 GD가 너무 오래걸린다.
A: SGD로 mini-batch마다 움직여 같은 시간에 훨씬 더 많이 진행해서 해결하자.

그러나, 미니 배치(mini-batch)를 하다보니 지그재그 방향 문제가 발생한다.
딱 봐도 더 잘 갈 수 있는데 훨씬 헤매면서 간다. 스텝사이즈(lr = learning rate)도 문제가 된다.
-> 더 잘 훑으면서, 더 좋은 방향으로 갈 수 있는 방법은?
보폭이 너무 작으면 오래 헤매고, 보폭이 너무 크면 좋은 방향을 지나칠 수 있다.

=> 어느 "방향"으로 발을 디딜지, 얼마 "보폭"으로 발을 디딜지 이 두 가지를 잘 잡아야 빠르게 내려올 수 있다.
SGD를 더 개선한 optimizer가 많다.
    - 산 내려오는 작은 오솔길 잘 찾기(Optimizer)
    GD: 모든 자료를 다 검토해서 내위치의 산 기울기를 계산해서 갈 방향을 찾겠다.
    SGD: 전부 다봐야 한걸음은 너무 오래 걸리니까 조금만 보고 빨리 판단한다. 같은 시간에 더 많이 간다.
    SGD -> Momentum : 스텝 계산해서 움직인 후, 아까 내려 오던 관성 방향 또 가자.
        -> Adagrad : 안가본곳은 성큼 빠르게 걸어 훑고 많이 가본 곳은 잘아니까 갈수록 보폭을 줄여 세밀히 탐색하자.

    Mometum -> NAG : 일단 관성 방향 먼저 움직이고, 움직인 자리에 스텝을 계산하니 더 빠르더라.
    
    Adagrad -> AdaDelta : 종종걸음 너무 작아져서 정지하는걸 막아보자
            -> RMSProp : 보폭을 줄이는 건 좋은데 이전맥략 상황을 봐가며 가자.

    Momentum + RMSProp -> Adam : RMSProp + Momentum 방향도 스텝사이즈도 적절하게 사용하자.

    NAG + Adam -> Nadam : Adam에 Momentum대신 NAG를 붙이자
    
    (잘 모르겠으면 Adam)

문제: SGD가 빠른데 좀 헤맨다?
해결: SGD의 개선된 버전을 골라 더 빠르고 더 정확하게!

●●●●    
3-> Overfitting 해결과정
Q: 뉴럴넷에게 융통성을 기르는 방법은?
    ex)
    열심히 뉴럴넷에게 고양이를 가르쳤더니?
    뚱뚱하니까 고양이가 아니다.
    갈색이니까 고양이가 아니다.
    귀쳐졌으니까 고양이가 아니다.
    -> 융통성이 없음. Overfitting
A: DropOut
    가르칠 때 부터, 좀 가리면서 가르치자.
    학습 시킬 때, 일부러 정보를 누락 시키거나 중간 중간 노드를 끈다.

dropout으로 일부에 집착하지 않고 중요한 요소가 무엇인지 터득해 나간다.
    ex)
    1. 고양이의 얼굴위주만 학습
    2. 고양이 img의 색을 지우고 
    3. 고양이 귀쪽을 빼고
    등등

문제: 과적합으로 융통성이 없다.
해결: DropOut으로 유연성을 획득시킨다.



